{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import isdir, join\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import wave\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import csv\n",
    "from pydub import AudioSegment as am"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paths_and_labels_to_dataset(audio_paths, labels):\n",
    "    \"\"\"Constructs a dataset of audios and labels.\"\"\"\n",
    "    path_ds = tf.data.Dataset.from_tensor_slices(audio_paths)\n",
    "    audio_ds = path_ds.map(lambda x: path_to_audio(x))\n",
    "    label_ds = tf.data.Dataset.from_tensor_slices(labels)\n",
    "    return tf.data.Dataset.zip((audio_ds, label_ds))\n",
    "\n",
    "\n",
    "def path_to_audio(path):\n",
    "    \"\"\"Reads and decodes an audio file.\"\"\"\n",
    "    audio = tf.io.read_file(path)\n",
    "    audio, _ = tf.audio.decode_wav(audio, 1,16000)\n",
    "    return audio\n",
    "def audio_to_fft(audio):\n",
    "    # Since tf.signal.fft applies FFT on the innermost dimension,\n",
    "    # we need to squeeze the dimensions and then expand them again\n",
    "    # after FFT\n",
    "    audio = tf.squeeze(audio, axis=-1)\n",
    "    fft = tf.signal.fft(\n",
    "        tf.cast(tf.complex(real=audio, imag=tf.zeros_like(audio)), tf.complex64)\n",
    "    )\n",
    "    fft = tf.expand_dims(fft, axis=-1)\n",
    "\n",
    "    # Return the absolute value of the first half of the FFT\n",
    "    # which represents the positive frequencies\n",
    "    return tf.math.abs(fft[:, : (audio.shape[1]), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "json_file = open(\"./model/model.json\", 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = keras.models.model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "model.load_weights(\"./model/model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "model.compile(\n",
    "    optimizer=\"Adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './recheck model'\n",
    "sound = am.from_file('./predict speaker/Chi.m4a', format='m4a')\n",
    "if os.path.exists(path+'/Han/'):\n",
    "    shutil.rmtree(path+'/Han/')\n",
    "os.mkdir(path+'/Han',)\n",
    "sound = sound.set_frame_rate(16000)\n",
    "for j in range(0,int(sound.frame_count() / 16000)-1):\n",
    "    Audio = sound[1000*j:(j+1)*1000]\n",
    "    m = path+'/Han/'+str(j)\n",
    "    Audio.export(m+'.wav', format=\"wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_ds = paths_and_labels_to_dataset([path+'/Han/{}.wav'.format(i) for i in range(0,int(sound.frame_count() / 16000)-1)], [4]*int(sound.frame_count() / 16000))\n",
    "audio_ds = audio_ds.shuffle(buffer_size=128 * 8, seed=65).batch(\n",
    "    128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = [\"Bao Han\",\"Thanh Chi\",\"Duc Manh\",\"Minh Hieu\",\"Gia Minh\",\"Noise\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thanh Chi 0.99674356\n",
      "Thanh Chi 0.58220667\n",
      "Thanh Chi 0.999982\n",
      "Bao Han 0.76517946\n",
      "Bao Han 0.50286937\n",
      "Bao Han 0.9999529\n",
      "Thanh Chi 0.94225734\n",
      "Thanh Chi 0.99999034\n"
     ]
    }
   ],
   "source": [
    "for a,b in audio_ds.take(1):\n",
    "  ffts = audio_to_fft(a)\n",
    "  y_pred = model.predict(ffts)\n",
    "for i in range(y_pred.shape[0]):\n",
    "  print(label[np.where(y_pred[i] == y_pred[i].max())[0][0]],y_pred[i].max())\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0f7afcab5d606588e322b620bd2cf2e61e7542c683b0032a65bcaa1833e026b2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
